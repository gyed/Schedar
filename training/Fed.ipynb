{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19afc2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.tool import GPU_info, write_log, print_label_stat\n",
    "from models.resnet import ResNet8, ResNet18, ResNet50\n",
    "from models.mobilenet import MobileNet_S, MobileNet_M, MobileNet_L\n",
    "from framework.ours import Device\n",
    "from framework.DSGD import DSGD_Device\n",
    "from framework.SISA import SISA_Device\n",
    "from framework.FedAvgUnl import FedAvgServer, FedAvgClient\n",
    "\n",
    "\n",
    "# one in ['ours', 'DSGD', 'SISA', 'Fed']\n",
    "Framework = 'Fed'\n",
    "\n",
    "# one in ['MNIST', 'FMNIST', 'CIFAR10']\n",
    "DatasetName = 'FMNIST'\n",
    "\n",
    "# one in ['resnet', 'mobilenet']\n",
    "ModelType = 'mobilenet'\n",
    "\n",
    "# one in [1,0]. 1 - Heterogeneous;  0 - Homogeneous\n",
    "Heterogeneous = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dd285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_o = None\n",
    "test_set_o = None\n",
    "num_classes = 10\n",
    "device_num = 6\n",
    "num_channel = 1\n",
    "ref_size = 10000\n",
    "train_test_total_size = int(60000/device_num)\n",
    "test_ratio = 0.2\n",
    "CIFAR10_segmentation = 0\n",
    "\n",
    "train_batch_size = 256 if ModelType == 'resnet' else 32\n",
    "save_path = './checkpoint'\n",
    "data_path = '../data'\n",
    "log_path = '../log/{}_{}_{}.txt'.format(Framework, ModelType, DatasetName)\n",
    "\n",
    "my_seed = 1\n",
    "torch.cuda.manual_seed(my_seed)\n",
    "epoch_num = 100\n",
    "\n",
    "if DatasetName == 'CIFAR10':\n",
    "    train_set_o = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "    test_set_o = datasets.CIFAR10(data_path, train=False, download=True)\n",
    "    device_num = 5\n",
    "    num_channel = 3\n",
    "    CIFAR10_segmentation = 1\n",
    "    train_test_total_size = int(50000/device_num)\n",
    "    epoch_num = 400\n",
    "    \n",
    "\n",
    "elif DatasetName == 'MNIST':    \n",
    "    train_set_o = datasets.MNIST(data_path, train=True, download=True)\n",
    "    test_set_o = datasets.MNIST(data_path, train=False, download=True)\n",
    "    epoch_num = 100\n",
    "\n",
    "elif DatasetName == 'FMNIST':    \n",
    "    train_set_o = datasets.FashionMNIST(data_path, train=True, download=True)\n",
    "    test_set_o = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    epoch_num = 300\n",
    "\n",
    "train_set_o.transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "test_set_o.transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "ref_set = Subset(test_set_o, range(0,int(ref_size)))\n",
    "ref_loader = torch.utils.data.DataLoader(ref_set, batch_size=train_batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2a8638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\tClass0\tClass1\tClass2\tClass3\tClass4\tClass5\tClass6\tClass7\tClass8\tClass9\tSUM\n",
      "D-0\t0\t720\t676\t687\t644\t658\t660\t676\t662\t663\t6046\n",
      "D-1\t669\t0\t643\t659\t681\t663\t697\t637\t659\t691\t5999\n",
      "D-2\t687\t637\t0\t667\t665\t701\t697\t672\t632\t634\t5992\n",
      "D-3\t658\t645\t647\t0\t638\t689\t682\t677\t718\t631\t5985\n",
      "D-4\t682\t654\t638\t652\t0\t643\t657\t676\t696\t686\t5984\n",
      "D-5\t634\t676\t706\t652\t683\t0\t660\t656\t675\t670\t6012\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\n",
    "device_dict = {}\n",
    "loader_dict = {}\n",
    "print('Device', end='\\t')\n",
    "for class_id in range(num_classes):\n",
    "    print('Class'+str(class_id), end='\\t')\n",
    "print('SUM')\n",
    "\n",
    "for device_id in range(device_num):\n",
    "    range_start = train_test_total_size * device_id\n",
    "    range_end = range_start + train_test_total_size\n",
    "    \n",
    "    # remove one class from each local dataset\n",
    "    class_to_remove = torch.tensor(device_id%10)\n",
    "    indices = (torch.tensor(train_set_o.targets[range_start:range_end])[..., None] !=\n",
    "               class_to_remove).any(-1).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # split train&test\n",
    "    train_test_border = int((1-test_ratio)*len(indices))\n",
    "    train_set = Subset(train_set_o, indices[:train_test_border]+range_start)\n",
    "    test_set = Subset(train_set_o, indices[train_test_border:]+range_start)\n",
    "    train_loader = DataLoader(train_set, batch_size=train_batch_size,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_set, batch_size=train_batch_size,\n",
    "                             shuffle=True, num_workers=0)\n",
    "    \n",
    "    loader_dict[device_id] = [train_loader, test_loader]\n",
    "    print_label_stat(device_id, train_set, num_classes)\n",
    "    \n",
    "# initialize devices\n",
    "for device_id in range(device_num):\n",
    "    gpu_id = 0\n",
    "    device_dict[device_id] = FedAvgClient(device_id, gpu_id=gpu_id, num_classes=num_classes, num_channel=num_channel)\n",
    "    \n",
    "Fed_server = FedAvgServer(Device_id_list = device_dict.keys(), gpu_id=gpu_id, num_classes=num_classes, num_channel=num_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d4cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heterogenerous scenario \n",
    "if Heterogeneous == 1:\n",
    "    # heterogeneous scenario\n",
    "    write_log(log_path, 'heterogeneous')\n",
    "    for k, v in device_dict.items():\n",
    "        v.client_model = ResNet8(num_channel) if ModelType == 'resnet' else MobileNet_S(num_channel)\n",
    "        v.client_model.cuda(v.gpu_id)\n",
    "        v.optimizer = optim.Adam(v.client_model.parameters(), lr=0.01)\n",
    "    Fed_server.central_model = ResNet8(num_channel) if ModelType == 'resnet' else MobileNet_S(num_channel)\n",
    "    Fed_server.central_model.cuda(Fed_server.gpu_id)\n",
    "else:\n",
    "    # homogeneous scenario\n",
    "    write_log(log_path, 'homogeneous')\n",
    "    for k, v in device_dict.items():\n",
    "        v.client_model = ResNet50(num_channel) if ModelType == 'resnet' else MobileNet_L(num_channel)\n",
    "        v.client_model.cuda(v.gpu_id)\n",
    "        v.optimizer = optim.Adam(v.client_model.parameters(), lr=0.01)\n",
    "    Fed_server.central_model = ResNet50(num_channel) if ModelType == 'resnet' else MobileNet_L(num_channel)\n",
    "    Fed_server.central_model.cuda(Fed_server.gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc32737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0\tEpoch:  0\t\tLoss: 0.74684054\n",
      "Client 1\tEpoch:  0\t\tLoss: 0.73502612\n",
      "Client 2\tEpoch:  0\t\tLoss: 0.25984645\n",
      "Client 3\tEpoch:  0\t\tLoss: 2.48468733\n",
      "Client 4\tEpoch:  0\t\tLoss: 0.48650038\n",
      "Client 5\tEpoch:  0\t\tLoss: 0.59811240\n",
      "Server - Avg_loss: 0.0732, Acc: 157.0/1512 (0.1038)\n",
      "Server - Avg_loss: 0.0720, Acc: 151.0/1500 (0.1007)\n",
      "Server - Avg_loss: 0.0723, Acc: 179.0/1499 (0.1194)\n",
      "Server - Avg_loss: 0.0725, Acc: 165.0/1497 (0.1102)\n",
      "Server - Avg_loss: 0.0726, Acc: 0.0/1497 (0.0000)\n",
      "Server - Avg_loss: 0.0722, Acc: 135.0/1503 (0.0898)\n",
      "epoch: 0 mean: 0.0873\t0.0099\t0.0915\t0.0176\t\n",
      "Client 0\tEpoch:  0\t\tLoss: 0.82807499\n",
      "Client 1\tEpoch:  0\t\tLoss: 1.21552014\n",
      "Client 2\tEpoch:  0\t\tLoss: 1.08323014\n",
      "Client 3\tEpoch:  0\t\tLoss: 1.04616618\n",
      "Client 4\tEpoch:  0\t\tLoss: 0.63886571\n",
      "Client 5\tEpoch:  0\t\tLoss: 0.85985363\n",
      "Server - Avg_loss: 0.0731, Acc: 165.0/1512 (0.1091)\n",
      "Server - Avg_loss: 0.0717, Acc: 150.0/1500 (0.1000)\n",
      "Server - Avg_loss: 0.0724, Acc: 164.0/1499 (0.1094)\n",
      "Server - Avg_loss: 0.0725, Acc: 154.0/1497 (0.1029)\n",
      "Server - Avg_loss: 0.0728, Acc: 183.0/1497 (0.1222)\n",
      "Server - Avg_loss: 0.0722, Acc: 174.0/1503 (0.1158)\n",
      "epoch: 1 mean: 0.1099\t0.0125\t0.1091\t0.0220\t\n",
      "Client 0\tEpoch:  0\t\tLoss: 0.61004889\n",
      "Client 1\tEpoch:  0\t\tLoss: 0.73893666\n",
      "Client 2\tEpoch:  0\t\tLoss: 1.37791145\n",
      "Client 3\tEpoch:  0\t\tLoss: 4.66663790\n",
      "Client 4\tEpoch:  0\t\tLoss: 0.73957002\n",
      "Client 5\tEpoch:  0\t\tLoss: 0.49943203\n",
      "Server - Avg_loss: 0.0769, Acc: 165.0/1512 (0.1091)\n",
      "Server - Avg_loss: 0.0748, Acc: 150.0/1500 (0.1000)\n",
      "Server - Avg_loss: 0.0772, Acc: 164.0/1499 (0.1094)\n",
      "Server - Avg_loss: 0.0749, Acc: 154.0/1497 (0.1029)\n",
      "Server - Avg_loss: 0.0774, Acc: 183.0/1497 (0.1222)\n",
      "Server - Avg_loss: 0.0749, Acc: 174.0/1503 (0.1158)\n",
      "epoch: 2 mean: 0.1099\t0.0125\t0.1110\t0.0221\t\n",
      "Client 0\tEpoch:  0\t\tLoss: 0.49915460\n",
      "Client 1\tEpoch:  0\t\tLoss: 0.76201099\n",
      "Client 2\tEpoch:  0\t\tLoss: 0.35647804\n",
      "Client 3\tEpoch:  0\t\tLoss: 2.98126125\n",
      "Client 4\tEpoch:  0\t\tLoss: 0.64545572\n",
      "Client 5\tEpoch:  0\t\tLoss: 0.62993944\n",
      "Server - Avg_loss: 0.1160, Acc: 304.0/1512 (0.2011)\n",
      "Server - Avg_loss: 0.1141, Acc: 258.0/1500 (0.1720)\n",
      "Server - Avg_loss: 0.1196, Acc: 273.0/1499 (0.1821)\n",
      "Server - Avg_loss: 0.1099, Acc: 258.0/1497 (0.1723)\n",
      "Server - Avg_loss: 0.1210, Acc: 278.0/1497 (0.1857)\n",
      "Server - Avg_loss: 0.1078, Acc: 281.0/1503 (0.1870)\n",
      "epoch: 3 mean: 0.1834\t0.0418\t0.1866\t0.0659\t\n",
      "Client 0\tEpoch:  0\t\tLoss: 0.44599453\n",
      "Client 1\tEpoch:  0\t\tLoss: 0.47328350\n",
      "Client 2\tEpoch:  0\t\tLoss: 0.49636513\n",
      "Client 3\tEpoch:  0\t\tLoss: 1.24147749\n",
      "Client 4\tEpoch:  0\t\tLoss: 0.35654134\n",
      "Client 5\tEpoch:  0\t\tLoss: 0.47652981\n",
      "Server - Avg_loss: 0.0779, Acc: 392.0/1512 (0.2593)\n",
      "Server - Avg_loss: 0.0808, Acc: 290.0/1500 (0.1933)\n",
      "Server - Avg_loss: 0.0778, Acc: 373.0/1499 (0.2488)\n",
      "Server - Avg_loss: 0.0790, Acc: 341.0/1497 (0.2278)\n",
      "Server - Avg_loss: 0.0784, Acc: 376.0/1497 (0.2512)\n",
      "Server - Avg_loss: 0.0718, Acc: 380.0/1503 (0.2528)\n",
      "epoch: 4 mean: 0.2389\t0.1250\t0.2403\t0.1234\t\n",
      "Client 0\tEpoch:  0\t\tLoss: 0.48047495\n",
      "Client 1\tEpoch:  0\t\tLoss: 0.84125656\n"
     ]
    }
   ],
   "source": [
    "write_log(log_path, time.ctime(time.time()))\n",
    "write_log(log_path, 'AvgAcc\\tAvgPre\\tAvgRec\\tAvgf1')\n",
    "for epoch in range(epoch_num):\n",
    "    for client_id, client in device_dict.items():\n",
    "        client.update_client_model(num_iter=3, local_loader=loader_dict[client_id][0])\n",
    "    Fed_server.update_central_model(frac=1, device_dict=device_dict)\n",
    "    Fed_server.distribute_central_model(device_dict=device_dict)\n",
    "    metric = []\n",
    "    for client_id, client in device_dict.items():\n",
    "        metric.append(Fed_server.validate_central_model(test_loader=loader_dict[client_id][1]))\n",
    "    metric_arr=np.array(metric)\n",
    "    log_txt = '{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t'.format(np.mean(metric_arr, axis=0)[0], \n",
    "                                                        np.mean(metric_arr, axis=0)[1],\n",
    "                                                        np.mean(metric_arr, axis=0)[2],\n",
    "                                                        np.mean(metric_arr, axis=0)[3])\n",
    "    print('epoch: {} mean: {}'.format(epoch, log_txt))\n",
    "    write_log(log_path, log_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217697c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c33ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b2b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
